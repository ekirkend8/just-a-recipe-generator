{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b871688a-3e96-4416-80d3-697fcf76a2cf",
   "metadata": {},
   "source": [
    "* https://huggingface.co/blog/how-to-generate\n",
    "* http://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584bbc0d-3322-443b-8dce-ccf898f26599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43bcc93d-e279-4a7b-86ee-49346f1ca276",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131cb87c-be33-4491-bcbc-f6847e931428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/EGK1/Documents/github/just-a-recipe-generator\n"
     ]
    }
   ],
   "source": [
    "cd ~/Documents/github/just-a-recipe-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "130751fe-44f3-42dd-a208-9ce79b4452d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013807058334350586,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 49,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 0,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9369df5b830f49359d0e90cae37f30c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b72f23-b762-4f90-9aa3-3311a0700bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/EGK1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/EGK1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from src.data.pickling import load_pickle\n",
    "from src.features.clean_shoestring_data import clean_shoestring_recipes\n",
    "from src.features.prepare_model_data import (ingredients_to_text, tokenize_text,\n",
    "                                             create_sequences, split_input_target,\n",
    "                                             make_training_data, text_from_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6ac67b-7414-40f0-a994-163b195fd158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/raw/shoestring_recipes.pickle for consumption...\n"
     ]
    }
   ],
   "source": [
    "shoestring_recipes0 = load_pickle(\"data/raw/shoestring_recipes.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e21b673-77c9-41b0-8e6c-babb846e3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoestring_recipes = copy.deepcopy(shoestring_recipes0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bb4504-0131-4553-9009-36b9bdf62e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_dict = clean_shoestring_recipes(shoestring_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3557a9bc-5ed3-4ddc-a266-7367399f2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ingredients_to_text(recipe_dict)\n",
    "# vocab, ids_from_chars, chars_from_ids = tokenize_text(text)\n",
    "# sequences = create_sequences(text, ids_from_chars)\n",
    "# dataset0 = sequences.map(split_input_target)\n",
    "# dataset = make_training_data(dataset0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fac00bc-eaf2-4b47-ad51-0805e8dd21d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ingredients:\\n 5.0 ounces canned pumpkin puree\\n 0.5'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98502c53-6f99-4319-89f4-095d71207ce8",
   "metadata": {},
   "source": [
    "## Encode context and generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a773a8-7740-4e57-8e52-aa921e8832c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 14:23:55.020806: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-10-11 14:23:55.021409: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load and init model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "564f693c-a883-4fdf-b7dd-fbca845af0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf', max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db29c9-1b44-4835-96d0-94060c3d0be0",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "A decoder method\n",
    "* Reduces the risk of missing hidden high prob word sequences by keeping the most likely number of beams of hypotheses at each time step and choosing the hypothesis that has the overall highest probability. \n",
    "* Always finds output sequence with higher prob than greedy search\n",
    "* Not guaranteed to find most likely output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ca0744-33d0-4d13-896b-0299502d7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_length=3987,\n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, \n",
    "    early_stopping=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4165ec1-a193-42d9-914c-0a29d7892b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
