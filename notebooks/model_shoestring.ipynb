{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3448caa",
   "metadata": {},
   "source": [
    "* https://medium.com/analytics-vidhya/generating-a-dessert-ingredients-list-12edd1740753#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImFjYjZiZTUxZWZlYTZhNDE5ZWM5MzI1ZmVhYTFlYzQ2NjBmNWIzN2MiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2NDU3NTE3MjYsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjExMjg5ODkxMzg4OTk3MTcxMDQ0OSIsImVtYWlsIjoia2lya2VuZGFsbC5ldmVAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF6cCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsIm5hbWUiOiJFdmVseW4gS2lya2VuZGFsbCIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQVRYQUp5Y3lzVGVLUkdBOS1EcW40dnNsZWFZb0VfNnVkV1Q5WW9qcFBTMD1zOTYtYyIsImdpdmVuX25hbWUiOiJFdmVseW4iLCJmYW1pbHlfbmFtZSI6IktpcmtlbmRhbGwiLCJpYXQiOjE2NDU3NTIwMjYsImV4cCI6MTY0NTc1NTYyNiwianRpIjoiNzM3ZDEzNWU2NTk0YzZmNTZlMzNhNjJjYWUyNzZkMzk0ZmNkOWNmYiJ9.P-jixfwiL_qANHTPar367nMcFUyINo-r3S7rv53JhaUdPek3yLzDIJOKaTd9-KLeEFufhC0ZDJXZmGQM0_5y1LEy5bDmzXnR5k3oZSEzMgohs0KMByovafuELy9Fl1PpnFakGSc81ZAgM3CSFNAyI3nqr3uBqnRmEFciFpTH72O71hROkQvpV7tBEhcKHS9w3LG8YyKmhLbFlcHqV-3QZITHkOwTfIGtAwg8eBm1ptVgf7Wi6GZfF_NI3EamPm8il4uhz8Wcn1URaC3MTqS8Z6Vlxsey_E9GGyTlv3fbyeoEYUF-B4VAcu1IFiMapxb5cAYEGVY0RiABB-yEsaNvrA\n",
    "* https://towardsdatascience.com/using-machine-learning-to-generate-recipes-that-actually-works-b2331c85ab72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0e2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e0a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/just-a-recipe-generator\n"
     ]
    }
   ],
   "source": [
    "cd /home/ec2-user/SageMaker/just-a-recipe-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6523435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1c8fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56ad9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.pickling import load_pickle\n",
    "from src.features.clean_shoestring_data import clean_shoestring_recipes\n",
    "from src.features.prepare_model_data import (ingredients_to_text, tokenize_text,\n",
    "                                             create_sequences, split_input_target,\n",
    "                                             make_training_data, text_from_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7a332896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.rnn import train_rnn_model, apply_one_step_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f122fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/raw/shoestring_recipes.pickle for consumption...\n"
     ]
    }
   ],
   "source": [
    "shoestring_recipes0 = load_pickle(\"data/raw/shoestring_recipes.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f91af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoestring_recipes = copy.deepcopy(shoestring_recipes0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699a36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_dict = clean_shoestring_recipes(shoestring_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f87a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ingredients_to_text(recipe_dict)\n",
    "vocab, ids_from_chars, chars_from_ids = tokenize_text(text)\n",
    "sequences = create_sequences(text, ids_from_chars)\n",
    "dataset0 = sequences.map(split_input_target)\n",
    "dataset = make_training_data(dataset0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d30da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 27s 3s/step - loss: 4.2418\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 4.1033\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 26s 3s/step - loss: 3.5450\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 2.9741\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 25s 3s/step - loss: 2.7141\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 25s 3s/step - loss: 2.4332\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 25s 3s/step - loss: 2.1991\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 26s 3s/step - loss: 2.0186\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 1.8815\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 1.7781\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 1.6700\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 1.5844\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.4881\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.3850\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 1.2878\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.1958\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 1.0976\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 1.0087\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.9392\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.8814\n"
     ]
    }
   ],
   "source": [
    "history, model = train_rnn_model(dataset, ids_from_chars, vocab, epochs=3)\n",
    "apply_one_step_model(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac556db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recipe title first? And from that, generate ingredients?\n",
    "# https://www.tensorflow.org/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec558f3d",
   "metadata": {},
   "source": [
    "## Create training batches\n",
    "\n",
    "Before feeding this data into the model, you need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0dc5b7",
   "metadata": {},
   "source": [
    "## Build The Model\n",
    "This section defines the model as a keras.Model subclass.\n",
    "\n",
    "This model has three layers:\n",
    "\n",
    "- tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
    "- tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
    "- tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.\n",
    "\n",
    "### References:\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/text_generation#build_the_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2249f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8de13d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "617f6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2081f397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60 61 63 41 78 41 48 23 13 55  0 67 61 26  0 45 38 56 42 22 35 69 28 32\n",
      " 32 47 61 32 15 76 24 34  9 43 44 67 52 73 17 70 43 30 18 45 20 51 69 76\n",
      " 68 14 10 33 51 46 51 53  1 48 43 42  4  8 67  6 77 63 76 35 21 12 10 56\n",
      " 53 10 77 16  9 19 13 35 79 63 38 42 60  3 19 27 35 18 13 31 57  3 51 75\n",
      " 70 15 15 55]\n",
      "Input:\n",
      " b'.75 cups all purpose gluten free flour blend\\n 1.5 teaspoons xanthan gum\\n 0.5 cup tapioca starch/flou'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'oprW\\xe2\\x85\\x93WcC4j[UNK]vpF[UNK]]TkYBPxHLLbpL6\\xc3\\xa8DN0Z[vg\\xc2\\xbc8yZJ9];fx\\xc3\\xa8w51Mfafh\\ncZY*/v-\\xe2\\x80\\x99r\\xc3\\xa8PA31kh1\\xe2\\x80\\x9970:4P\\xe2\\x85\\x9brTYo&:GP94Kl&f\\xc2\\xbey66j'\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "\n",
    "print(sampled_indices)\n",
    "\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0], chars_from_ids))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices, chars_from_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0851727",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc6916df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 80)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.383031, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8c1dc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.08038"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the exponential of the mean loss is approximately equal to the vocabulary size.\n",
    "# A much higher loss means the model is sure of its wrong answers, and is badly initialized\n",
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93ae285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab9addcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoints during training'\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"checkpoint_{epoch}\")\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4f50a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7991ee2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 27s 3s/step - loss: 4.2331\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 4.0945\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 27s 3s/step - loss: 3.4962\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 24s 3s/step - loss: 2.9806\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 2.7190\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 2.4382\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 2.2114\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 2.0263\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.8826\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.7721\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.6682\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 26s 3s/step - loss: 1.5569\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.4639\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.3660\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.2503\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.1750\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 26s 3s/step - loss: 1.0681\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.9877\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.9385\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.8693\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.8093\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.7752\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 27s 3s/step - loss: 0.7279\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.6853\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.6529\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.6165\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.5962\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.5632\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.5497\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.5334\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.5045\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.4817\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.4644\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.4511\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.4343\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.4197\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.4022\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.3930\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.3807\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 27s 3s/step - loss: 0.3675\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.3604\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.3484\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.3370\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.3268\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.3169\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.3118\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.3019\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.2932\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.2825\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.2765\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d54162f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    \"\"\"Make a single step prediction\"\"\"\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(\n",
    "            inputs=input_ids, states=states, return_state=True\n",
    "        )\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97a72222",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80a460bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients:\n",
      " 3.33 cups extra tian gruten purce canafle dark Bread milk\n",
      "\n",
      " Ingredients:\n",
      " 0.75 cup granulated sugar\n",
      " 0.75 cup granulated sugar\n",
      " 1.0 egg\n",
      " 1.0 egg whites\n",
      " 1.0 cup all purpose gluten-free plour blend\n",
      " 12.0 tablespoons unsalted butter\n",
      " Lukewarm water by the 0.5 teaspoon ground ciniamps butter\n",
      " 2.0 tablespoons & 310 ggum\n",
      " 0.35 cup um werteri\n",
      " 1.0 9.0 ounces cedtsated sugar\n",
      " 2.0 eggs\n",
      " 1.5 cups milk\n",
      " 4.0 ounces unsweetened chocolate\n",
      " 8.0 ounces seats chld exdrued riaces\n",
      " 2.0 teaspoons pure vanilla extract\n",
      " Seeaspoon xanthan gum\n",
      " 2.0 tablespoons unsalted butter\n",
      "\n",
      " Ingredients:\n",
      " 1.75 cups all-purpose gluten free flour blend\n",
      " 1.5 teaspoons baking powder\n",
      " 0.5 teaspoon kosher salt\n",
      " 2.0 teaspoons pure maple syrup\n",
      " 0.25 cup nuter\n",
      " 4.0 tablespoons unsulphured molasses\n",
      " 0.5 cup packed light brown sugar\n",
      " 1.0 egg\n",
      " 1.0 tablespoon granulated sugar\n",
      " 1.25 cups bloth rocolate chips\n",
      " 3.0 ounces enspack ole-sweetened chocolate\n",
      "\n",
      " Ingredients:\n",
      " 1.51cup glanu cupfree gruten free pasalt grated corndranch\n",
      " 2.0 tab \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.5948996543884277\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Ingredients:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffc07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "629d8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients: ons\n",
      " 1.0 tablespoons unsalted butthre froon bakinga\n",
      " 0.5 teaspoon froundstar bated\n",
      " 0.5 teaspoons pash baamed sheegly\n",
      "  grawterpinited sugar\n",
      " 5.75 teaspoons varing poweem glack\n",
      " 0.5 teaspoon kosher salt\n",
      " 0.5 p ] uras graked bietspor\n",
      " 0.0 tablespoon superdine frour chetpy-suct\n",
      " 1.5 tablespoons waniel sure froun baking soda\n",
      " 6.0 egg\n",
      " 1.2 teaspoon kosher oant yil\n",
      "\n",
      " 3.0 gables outter shion\n",
      " 0.5 teaspoon baking soda\n",
      " 0.12 t blaspoons yilind piundres batt\n",
      " 0.25 teaspoon kosh poanendi[ taried soanstare vamily\n",
      "\n",
      "\n",
      " Ingredients:\n",
      " 61.0 sugar\n",
      " 1.55 gem\n",
      " 1.0 gur\n",
      " 0.75 cup orens ounce\n",
      " 1.0 tablespoons unsweeterter sugar\n",
      " 0.0 tu7 loun yan fate coonstoonster batieg powder-Dis\n",
      " 1.0 teaspoon xanthant ctareg\n",
      " 0.25 teaspoons unsalted butter\n",
      " 1.0 peaspion parcer salt\n",
      " 1.0 tablespoons doterarmosa 0.5 cup our\n",
      " 8.0 tablespoons gi]d toamsves\n",
      " 0.75 cup w8.25 cup garp sales\n",
      "\n",
      " Ingredients:\n",
      " 2.0 tablespoons granu\n",
      " 0.5 cup packid poras all purspoond cornitict\n",
      " 4.0 8alled outterm [ terspoon xabthanets\n",
      " 3.83275 teasp \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.519045352935791\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <src.models.rnn.OneStep object at 0x7fae03b3a2d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "apply_one_step_model(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e10daf",
   "metadata": {},
   "source": [
    "## Advanced: Customized Training\n",
    "\n",
    "The above training procedure is simple, but does not give you much control. It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
    "\n",
    "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement curriculum learning to help stabilize the model's open-loop output.\n",
    "\n",
    "The most important part of a custom training loop is the train step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407afb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "model.fit(dataset, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p37",
   "language": "python",
   "name": "conda_tensorflow2_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
